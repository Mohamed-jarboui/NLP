{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration - Resume NER\n",
    "\n",
    "This notebook explores the generated NER dataset for resume keyword extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, val, test datasets\n",
    "data_dir = Path('../data/raw')\n",
    "\n",
    "with open(data_dir / 'train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(data_dir / 'val.json', 'r') as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "with open(data_dir / 'test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"Train: {len(train_data)} sentences\")\n",
    "print(f\"Val: {len(val_data)} sentences\")\n",
    "print(f\"Test: {len(test_data)} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample sentences with annotations\n",
    "for i, item in enumerate(train_data[:5]):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Sample {i+1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Text: {item['text']}\")\n",
    "    print(f\"\\nEntities:\")\n",
    "    for token, tag in zip(item['tokens'], item['tags']):\n",
    "        if tag != 'O':\n",
    "            print(f\"  {token} -> {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count all tags\n",
    "all_tags = []\n",
    "for item in train_data:\n",
    "    all_tags.extend(item['tags'])\n",
    "\n",
    "tag_counts = Counter(all_tags)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "tags = list(tag_counts.keys())\n",
    "counts = list(tag_counts.values())\n",
    "colors = ['#3498db' if 'SKILL' in t else '#2ecc71' if 'DEGREE' in t else '#e74c3c' if 'EXPERIENCE' in t else '#95a5a6' for t in tags]\n",
    "\n",
    "plt.bar(tags, counts, color=colors, alpha=0.7)\n",
    "plt.title('Tag Distribution in Training Set', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Tag', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count entities (B- tags only)\n",
    "entity_counts = {\n",
    "    'SKILL': tag_counts.get('B-SKILL', 0),\n",
    "    'DEGREE': tag_counts.get('B-DEGREE', 0),\n",
    "    'EXPERIENCE': tag_counts.get('B-EXPERIENCE', 0)\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Entity Type': list(entity_counts.keys()),\n",
    "    'Count': list(entity_counts.values())\n",
    "})\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "colors_pie = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "plt.pie(df['Count'], labels=df['Entity Type'], autopct='%1.1f%%', colors=colors_pie, startangle=90)\n",
    "plt.title('Entity Distribution', fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentence lengths\n",
    "sentence_lengths = [len(item['tokens']) for item in train_data]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sentence_lengths, bins=30, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "plt.title('Sentence Length Distribution', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Number of Tokens', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.axvline(x=sum(sentence_lengths)/len(sentence_lengths), color='red', linestyle='--', label=f'Mean: {sum(sentence_lengths)/len(sentence_lengths):.1f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Min length: {min(sentence_lengths)}\")\n",
    "print(f\"Max length: {max(sentence_lengths)}\")\n",
    "print(f\"Mean length: {sum(sentence_lengths)/len(sentence_lengths):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The dataset is well-balanced with good coverage of all three entity types. Ready for training!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
